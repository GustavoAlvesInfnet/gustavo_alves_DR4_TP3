import os

from langchain.chains import LLMChain
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
    AIMessagePromptTemplate,
)
from langchain_core.messages import SystemMessage
from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain_groq import ChatGroq

import json
import matplotlib.pyplot as plt

def ex01(pergunta):
    """
    This function is the main entry point of the application. It sets up the Groq client, the Streamlit interface, and handles the chat interaction.
    """

    # Get Groq API key
    groq_api_key = os.environ['GROQ_API_KEY']
    model = 'llama3-8b-8192'
    # Initialize Groq Langchain chat object and conversation
    groq_chat = ChatGroq(
            groq_api_key=groq_api_key, 
            model_name=model
    )
    
    print("Hello! I'm your friendly Groq chatbot. I can help answer your questions, provide information, or just chat. I'm also super fast! Let's start our conversation!")

    system_prompt = 'You are a friendly conversational chatbot'
    conversational_memory_length = 5 # number of previous messages the chatbot will remember during the conversation

    memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key="chat_history", return_messages=True)

    count = 0
    #chat_history = []
    while True:
        if count == 0:
            user_question = pergunta
            count += 1
        else:
            user_question = input("Ask a question: ")
            count +=1

        # If the user has asked a question,
        if user_question:

            example_ai = AIMessagePromptTemplate.from_template('''
            Classifique os seguintes comentários como "Positivos", "Neutros" ou "Negativos":

            Exemplos:
            - "Eu amo esse produto!" Resposta: (Positivo)
            - "Esse produto é ok, mas não é o melhor." Resposta:(Neutro)
            - "Eu odeio esse produto, é um lixo!" Resposta:(Negativo)
            - "Eu não sei se eu gostei desse produto." Resposta:(Neutro)
            - "Esse produto é incrível, eu o recomendo!" Resposta:(Positivo)
            - "Eu não tenho opinião sobre esse produto." Resposta:(Neutro)
            - "Esse produto é ruim, não compre!" Resposta:(Negativo)
            - "Eu gostei desse produto, mas não é perfeito." Resposta:(Positivo)
            - "Esse produto é médio, não é nada especial." Resposta:(Neutro)
            ''')

            # Construct a chat prompt template using various components
            prompt = ChatPromptTemplate.from_messages(
                [
                    SystemMessage(
                        content=system_prompt
                    ),  # This is the persistent system prompt that is always included at the start of the chat.

                    MessagesPlaceholder(
                        variable_name="chat_history"
                    ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.

                    HumanMessagePromptTemplate.from_template(
                        "{human_input}"
                    ),  # This template is where the user's current input will be injected into the prompt.

                    example_ai
                    
                ]
            )

            # Create a conversation chain using the LangChain LLM (Language Learning Model)
            conversation = LLMChain(
                llm=groq_chat,  # The Groq LangChain chat object initialized earlier.
                prompt=prompt,  # The constructed prompt template.
                verbose=False,   # TRUE Enables verbose output, which can be useful for debugging.
                memory=memory,  # The conversational memory object that stores and manages the conversation history.
            )
            # The chatbot's answer is generated by sending the full prompt to the Groq API.
            response = conversation.predict(human_input=user_question)
            print("Chatbot:", response)

#ex01("classifique a frase: Este episódio é divertido, mas não tão bom quanto os antigos.")



def ex02():
    """
    This function is the main entry point of the application. It sets up the Groq client, the Streamlit interface, and handles the chat interaction.
    """

    # Get Groq API key
    groq_api_key = os.environ['GROQ_API_KEY']
    model = 'llama3-8b-8192'
    # Initialize Groq Langchain chat object and conversation
    groq_chat = ChatGroq(
            groq_api_key=groq_api_key,
            model_name=model
    )

    print("Hello! I'm your friendly Groq chatbot. I can help answer your questions, provide information, or just chat. I'm also super fast! Let's start our conversation!")

    system_prompt = 'You are a friendly conversational chatbot'
    conversational_memory_length = 5 # number of previous messages the chatbot will remember during the conversation

    memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key="chat_history", return_messages=True)

    # Prompt para validar entrada
    validation_prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content="You are a friendly conversational chatbot. If the user's input contains any inappropriate or offensive language, respond with 'I don't know about that.'"),
        HumanMessagePromptTemplate.from_template("{human_input}"),
    ])

    # Crie uma nova conversa com o prompt de validação
    validation_conversation = LLMChain(llm=groq_chat, prompt=validation_prompt, verbose=False, memory=memory)

    while True:
        user_question = input("Ask a question: ")

        if user_question:
            # Construct a chat prompt template using various components
            prompt = ChatPromptTemplate.from_messages([
                SystemMessage(content=system_prompt),
                MessagesPlaceholder(variable_name="chat_history"),
                HumanMessagePromptTemplate.from_template(user_question),
            ])

            # Create a conversation chain using the LangChain LLM (Language Learning Model)
            conversation = LLMChain(llm=groq_chat, prompt=prompt, verbose=False, memory=memory)

            # The chatbot's answer is generated by sending the full prompt to the Groq API.
            response = conversation.predict(human_input=user_question)

            # Faça a validação da entrada
            validation_response = validation_conversation.predict(human_input=user_question)

            if validation_response.lower() == "i don't know about that.":
                print("Chatbot:", validation_response)
            else:
                print("Chatbot:", response)


#ex02()


def ex04(exemplo):
    """
    This function is the main entry point of the application. It sets up the Groq client, the Streamlit interface, and handles the chat interaction.
    """

    # Get Groq API key
    groq_api_key = os.environ['GROQ_API_KEY']
    model = 'llama3-8b-8192'
    # Initialize Groq Langchain chat object and conversation
    groq_chat = ChatGroq(
            groq_api_key=groq_api_key, 
            model_name=model
    )
    
    print("Hello! I'm your friendly Groq chatbot. I can help answer your questions, provide information, or just chat. I'm also super fast! Let's start our conversation!")

    system_prompt = 'You are a friendly conversational chatbot'
    conversational_memory_length = 5 # number of previous messages the chatbot will remember during the conversation

    memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key="chat_history", return_messages=True)


    #chat_history = []
    while True:
        user_question = input("Ask a question: ")

        # If the user has asked a question,
        if user_question:

            example_ai = AIMessagePromptTemplate.from_template(exemplo)

            # Construct a chat prompt template using various components
            prompt = ChatPromptTemplate.from_messages(
                [
                    SystemMessage(
                        content=system_prompt
                    ),  # This is the persistent system prompt that is always included at the start of the chat.

                    MessagesPlaceholder(
                        variable_name="chat_history"
                    ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.

                    HumanMessagePromptTemplate.from_template(
                        "{human_input}"
                    ),  # This template is where the user's current input will be injected into the prompt.

                    example_ai
                    
                ]
            )

            # Create a conversation chain using the LangChain LLM (Language Learning Model)
            conversation = LLMChain(
                llm=groq_chat,  # The Groq LangChain chat object initialized earlier.
                prompt=prompt,  # The constructed prompt template.
                verbose=False,   # TRUE Enables verbose output, which can be useful for debugging.
                memory=memory,  # The conversational memory object that stores and manages the conversation history.
            )
            # The chatbot's answer is generated by sending the full prompt to the Groq API.
            response = conversation.predict(human_input=user_question)
            print("Chatbot:", response)

exemplos_materias = {
    "Dólar dispara e bate recorde com notícias sobre isenção do IR":"negativo",
    "Deputados começam a recolher assinaturas para PEC alternativa":"neutro",
    "Chuva torrencial em Salvador causa deslizamentos e deixa 1 morto":"negativo",
    "Morte de mulher: médico que fez hidrolipo já foi processado 21 vezes":"negativo",
    "Mbappé e Salah perdem pênaltis, e Liverpool vence Real Madrid pela Champions": "neutro",
    "A incrível (e inspiradora) história do restaurante sem lixeira":"positivo",
    "Presidente do México teve “conversa excelente” com Trump sobre imigração":"positivo",
}

exemplos_materias2 ="""
Dólar dispara e bate recorde com notícias sobre isenção do IR:negativo, 
Deputados começam a recolher assinaturas para PEC alternativa:neutro, 
Chuva torrencial em Salvador causa deslizamentos e deixa 1 morto:negativo, 
Morte de mulher: médico que fez hidrolipo já foi processado 21 vezes:negativo, 
Mbappé e Salah perdem pênaltis, e Liverpool vence Real Madrid pela Champions:neutro, 
A incrível (e inspiradora) história do restaurante sem lixeira:positivo, 
Presidente do México teve “conversa excelente” com Trump sobre imigração:positivo

Responda no formato Json
"""
#ex04(exemplos_materias2)

def ex04_2():
    # Carregar o JSON
    with open('.\\data\\noticias.json') as f:
        noticias = json.load(f)

    # Contar o número de notícias por sentimento
    sentimentos = {}
    for noticia in noticias['noticias']:
        sentimento = noticia['sentimento']
        if sentimento in sentimentos:
            sentimentos[sentimento] += 1
        else:
            sentimentos[sentimento] = 1

    # Criar o gráfico de barras
    plt.bar(sentimentos.keys(), sentimentos.values())
    plt.xlabel('Sentimento')
    plt.ylabel('Número de Notícias')
    plt.title('Distribuição de Notícias por Sentimento')
    plt.show()

#ex04_2()

def ex05():
    import kagglehub

    # Download latest version
    path = kagglehub.dataset_download("prashant111/the-simpsons-dataset")

    print("Path to dataset files:", path)

    import pandas as pd

    # Leia os arquivos CSVs
    files = [path + "/simpsons_characters.csv", path + "/simpsons_episodes.csv", path + "/simpsons_locations.csv", path + "/simpsons_script_lines.csv"]	

    # Combinar os arquivos CSVs num único dataset
    df = pd.concat([pd.read_csv(file) for file in files], ignore_index=True)

    print(df.head())
    print(df.columns)


    import tiktoken

    # Obtenha uma instância da classe Encoding pré-definida
    encoding = tiktoken.get_encoding("cl100k_base")

    # Descreva a quantidade de tokens por episódios e temporada
    df['tokens'] = df['raw_text'].apply(lambda x: len(encoding.encode(str(x))))
    df['season_tokens'] = df.groupby('season')['tokens'].transform('sum')

    # Média de tokens por episódio
    mean_tokens_per_episode = df['tokens'].mean()

    # Média de tokens por temporada
    mean_tokens_per_season = df.groupby('season')['tokens'].mean().mean()

    # Temporada com mais tokens
    max_tokens_season = df.groupby('season')['tokens'].sum().idxmax()

    # Episódio com mais tokens
    max_tokens_episode = df.loc[df['tokens'].idxmax()]

    # Prompt 1: Qual é a avaliação média do IMDB por temporada?
    prompt1 = "Qual é a avaliação média do IMDB por temporada?"
    df['imdb_rating'] = df.groupby('season')['imdb_rating'].mean()

    # Prompt 2: Qual é a audiência média por temporada?
    prompt2 = "Qual é a audiência média por temporada?"

    # Prompt 3: Qual é a relação entre a avaliação do IMDB e a audiência por temporada?
    prompt3 = "Qual é a relação entre a avaliação do IMDB e a audiência por temporada?"
    correlations = df.groupby('season')[['imdb_rating', 'us_viewers_in_millions']].corr()

    print("Média de tokens por episódio:", mean_tokens_per_episode)
    print("Média de tokens por temporada:", mean_tokens_per_season)
    print("Temporada com mais tokens:", max_tokens_season)

def ex06():
    import pandas as pd
    import torch
    from transformers import AutoModelForSequenceClassification, AutoTokenizer

    import kagglehub

    # Download latest version
    path = kagglehub.dataset_download("prashant111/the-simpsons-dataset")

    print("Path to dataset files:", path)

    import pandas as pd

    # Leia os arquivos CSVs
    files = [path + "/simpsons_characters.csv", path + "/simpsons_episodes.csv", path + "/simpsons_locations.csv", path + "/simpsons_script_lines.csv"]
    df = pd.concat([pd.read_csv(file) for file in files], ignore_index=True)

    # Selecionar o episódio número 92 da temporada 5
    episode_id = 92
    season = 5
    df_episode = df[(df["season"] == season) & (df["episode_id"] == episode_id)]

    # Preparar os dados para o modelo
    prompts = []
    labels = []
    for index, row in df_episode.iterrows():
        prompt = row["spoken_words"]
        label = 0  # positivo
        if "bad" in prompt or "worst" in prompt:
            label = 2  # negativo
        elif "good" in prompt or "great" in prompt:
            label = 0  # positivo
        else:
            label = 1  # neutro
        prompts.append(prompt)
        labels.append(label)

    # Definir o modelo e o tokenizer
    model_name = "distilbert-base-uncased"
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Preparar os dados para o batch-prompting
    batch_size = 32
    batches = []
    for i in range(0, len(prompts), batch_size):
        batch_prompts = prompts[i:i+batch_size]
        batch_labels = labels[i:i+batch_size]
        batches.append((batch_prompts, batch_labels))

    # Realizar as chamadas ao LLM
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    results = []
    for batch in batches:
        batch_prompts, batch_labels = batch
        inputs = tokenizer(batch_prompts, return_tensors="pt", padding=True, truncation=True)
        inputs.to(device)
        outputs = model(**inputs)
        logits = outputs.logits.detach().cpu().numpy()
        predictions = torch.argmax(torch.tensor(logits), dim=1).numpy()
        results.extend(predictions)

    # Avaliar o modelo
    accuracy = sum([1 if result == label else 0 for result, label in zip(results, labels)]) / len(labels)
    print("Acurácia do modelo:", accuracy)

    # Avaliar a precisão do modelo para cada classe
    num_pos = sum([1 if label == 0 else 0 for label in labels])
    num_neu = sum([1 if label == 1 else 0 for label in labels])
    num_neg = sum([1 if label == 2 else 0 for label in labels])

    precision_pos = sum([1 if result == 0 and label == 0 else 0 for result, label in zip(results, labels)]) / num_pos if num_pos != 0 else 0
    precision_neu = sum([1 if result == 1 and label == 1 else 0 for result, label in zip(results, labels)]) / num_neu if num_neu != 0 else 0
    precision_neg = sum([1 if result == 2 and label == 2 else 0 for result, label in zip(results, labels)]) / num_neg if num_neg != 0 else 0

    print("Precisão do modelo para cada classe:")
    print("Positivo:", precision_pos)
    print("Neutro:", precision_neu)
    print("Negativo:", precision_neg)

    # Quantas chamadas ao LLM foram necessárias?
    print("Quantas chamadas ao LLM foram necessárias:", len(batches))

    # Qual é a distribuição de fala por categoria?
    print("Distribuição de fala por categoria:")
    print("Positivo:", num_pos)
    print("Neutro:", num_neu)
    print("Negativo:", num_neg)





while True:
    print("Qual exercicio deseja executar?")
    print("1, 2, 4, 5, 6 ou 10?")
    exercicio = int(input())
    if exercicio == 1:
        ex01("classifique a frase: Este episódio é divertido, mas não tão bom quanto os antigos.")
        break
    elif exercicio == 2:
        ex02()
        break
    elif exercicio == 4:
        ex04()
        ex04_2()
        break
    elif exercicio == 5:
        ex05()
        break
    elif exercicio == 6:
        ex06()
        break
    elif exercicio == 10:
        # usa o comando sreamlit streamlit run .\main\LC.py
        import subprocess
        comando = "streamlit run .\\main\\streamlit.py"
        subprocess.run(comando, shell=True)
        break
    else:
        print("Opção inválida. Tente novamente.")